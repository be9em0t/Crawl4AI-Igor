# Project description
Documentation extraction crawler that uses crawl4ai and its LLM capability.

# Prerequisits
Crawl4ai has been set and available in the local pyenv. 
YAML files store information about different craml modes, like name, LLM model, Instruction etc.  
There is always LLM available during crawl. If not - warn the user and exit.
The testing is always done with live data and network access, as monkeypatching yields falso positive results.

The documentation of crawl4ai is available here #fetch https://docs.crawl4ai.com/.

## Example Crawl Mode implementation guidelines
- Mode name is "Unity ShaderGraph"
- URL is "https://docs.unity3d.com/Packages/com.unity.shadergraph@17.4/manual/Node-Library.html" 
- Capturing requirements:
  - Node library has topics, categories and nodes
  - we want to capture all nodes in the correct category structure
  - a category always contains nodes, otherwise it's just a topic
  - a topic can have zero or more categories 
  - if a topic has zero categories it itself is the category
Use this example as a stating point.
Graph nodes:
Artistic topic:
  - Adjustment category:
    - Channel Mixer node
    - Contast node
    - ...
  - Blend category
    - ...
  - Filter category
    - ...
  - Mask category
    - ...
  - Normal category
    - ...
  - Utility category
    - ...
Channel topic = Channel category:
  - Combine Channels
  - Split Channels
  - Reorder Channels
  - Flip Channels
Custom Render Texture topic = Custom Render Texture category:
  - ... 
Input topic = Input category:
  - ... 
Math topic = Math category:
  - ...
Procedural topic = Procedural category:
  - ...
Utility topic = Utility category:
  - ...
UV topic = UV category:
  - ...

## Running the commands
Basic run:
python3 crawl_cat.py <yaml file>
Help:
python3 crawl_cat.py --help

## Configuration and provider/token overrides
This tool expects a single site-specific YAML file as the positional argument (the `crawl_yaml`).
Each YAML contains site settings such as `url`, `provider` (optional), `structure_instruction`, and `content_instruction`.

Provider and token resolution order:
- The positional YAML can specify a `provider` (for example `openai/gpt-4.1`), but API tokens are read from the environment (never from the YAML).
- To provide an API token, set one of the environment variables:
  - `OPENAI_API_KEY` for OpenAI providers
  - `ANTHROPIC_API_KEY` for Anthropic providers
  - `HUGGINGFACE_API_TOKEN` for Hugging Face providers
- You can also override the provider or token on the CLI:
  - `--provider <provider-name>` to override the provider
  - `--api-token <token>` to pass a token directly (useful for one-off runs)

If a hosted provider is selected but no token is found in the environment or via `--api-token`, the script will refuse to run the LLM content pass. If you requested content but have no token, the script will fall back to DOM-based deterministic extraction and heuristics.

## Default CLI crawl4ai implementatin:
crwl https://www.sidefx.com/docs/houdini20.5/nodes/obj/index.html -q "Extract node names and attached node descriptions" --output json --output-file houdini_crwl.json

## LLM tuning guidance (quick)

When a crawl uses an LLM (`type: "llm"`), you can tune call behavior via the YAML `params` block. Keep these tips short and practical:

- `temperature` (float): controls randomness. Use `0.0` for deterministic structure extraction. Small values like `0.0 - 0.2` are recommended.
- `max_tokens` (int): budget for the LLM response. For structure-only JSON extracts 512–1024 is often enough. Large values (e.g. 3000) are only needed for very verbose outputs and can approach model context limits.
- `chunk_token_threshold` (int): page token size above which the page is split into chunks. Increase this to reduce the number of LLM calls for large documentation pages if your model has a large context window (e.g. 8000 or 24000). Lower values make more, smaller requests.
- `apply_chunking` (bool): enable or disable chunking. Disable only if you are sure the entire page fits comfortably in your model's context window.
- `overlap_rate` (float 0.0-1.0): small overlap (0.05–0.15) between chunks helps avoid missed items across chunk boundaries but increases token usage.

Example `params` tuned for a large docs site when using a model with an 8k+ token window:

```yaml
params:
  temperature: 0.0
  max_tokens: 1024
  chunk_token_threshold: 8000
  apply_chunking: true
  overlap_rate: 0.1
```

If you use a model with a larger context (24k–32k), consider increasing `chunk_token_threshold` accordingly and keep `max_tokens` conservative so prompt + response stay within the model's context limit.

Note: provider/model context windows differ. Check your provider's model docs (OpenAI/Anthropic/HuggingFace) and tune `chunk_token_threshold` and `max_tokens` so prompt_size + max_tokens < model_context_size (leave a small margin).

## Prompting for crawl4ai (quick)

Crawl4ai's LLM-driven extraction is prompt-sensitive. Keep prompts short, explicit, and machine-parseable. These short, practical guidelines are tuned for reliable structure and DOM discovery:

- Start with a JSON schema or explicit structure example. If you provide a schema, request the LLM to return only JSON that conforms to it. This reduces hallucinations and parsing edge-cases.
  Example: "Return a strict JSON array of topic objects. Each topic must have 'topic' (string) and 'categories' (array). Each category must have 'category' and 'nodes' (array of strings). Return only valid JSON."
- When asking for DOM selectors, ask for both index-page selectors and node-page selectors separately and insist on returning a strict JSON object with named fields `index_selectors` and `node_page_selectors`.
- Prioritize deterministic selectors first: request the LLM to prefer stable IDs, named classes, or anchor link patterns (e.g., `#main .node-list a`), and fall back to heading+paragraph heuristics as a secondary option.
- Include corrective examples when the schema can vary. Show one or two valid minimal examples inline so the LLM can mimic the shape precisely.
- Ask the model to avoid extraneous text: "Only return JSON, no commentary, no Markdown." This makes extraction and validation deterministic.
- Validate the LLM output immediately: parse the JSON and run a quick shape-check. If parsing fails, retry once with an instruction to strictly conform to the schema.

Short prompt pattern (compact):

"Instruction: Return a strict JSON array matching this schema: [example schema here]. Return only valid JSON. If any field is missing, return an empty string for it."

These conventions keep crawl4ai runs predictable and make downstream merging/heuristics (DOM scraping and enrichment) simpler.

## Schema (quick)

The project expects a simple Crawl Mode schema: a top-level JSON array of "topic" objects. Each topic has a `topic` string and a `categories` array. Each category has a `category` string and `nodes`, which is an array of node names (strings) or node objects when enriched.

Minimal schema example (structure-only):

```json
[
  {
    "topic": "Artistic",
    "categories": [
      {"category": "Adjustment", "nodes": ["Channel Mixer", "Contrast"]},
      {"category": "Blend", "nodes": ["Blend Node A", "Blend Node B"]}
    ]
  },
  {"topic": "Channel", "categories": [{"category": "Channel", "nodes": ["Combine Channels", "Split Channels"]}]}
]
```

Enriched output (nodes include descriptions):

```json
[
  {
    "topic": "Artistic",
    "categories": [
      {
        "category": "Adjustment",
        "nodes": [
          {"name": "Channel Mixer", "description": "Mixes color channels together."},
          {"name": "Contrast", "description": "Adjusts image contrast."}
        ]
      }
    ]
  }
]
```

Unity ShaderGraph mapping note: the top-level "topics" correspond to the major sections of the ShaderGraph Node Library (e.g., "Artistic", "Channel"), categories are the sub-groups within those sections, and nodes are the individual node names. During enrichment, nodes become objects with `name` and `description` fields.
